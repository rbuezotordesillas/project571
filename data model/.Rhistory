mean(defaulted) # we obtained that only 12% of the cases would we yes
mean(test$default_payment_next_month)
#The mean of "yes" is in both cases really small, since there are low number of examples that
#has 1 as an outcome.
mean(defaulted==test$default_payment_next_month)
#The performance of the model is quite good as it has an accuracy of 0.82 which is quite high.
#If we take a look at the distribution
TP<-table(defaulted, test$default_payment_next_month)[4]
FP<-table(defaulted, test$default_payment_next_month)[2]
FN<-table(defaulted, test$default_payment_next_month)[3]
recall<-TP/(TP+FN)
precision<-TP/(TP+FP)
#We have a recall of 0.36 and a precision of 0.68. In both cases they are quite low
log_aic<-AIC(logmodel)
library(MASS)
library(stats)
#library(leaps)
simple<-glm(default_payment_next_month ~ 1,family=binomial(link='logit'), data=credit_clean)
full<-glm(default_payment_next_month ~.,family=binomial(link='logit'), data=credit_clean)
forwmodel<-step(simple, direction="forward", scope=list(upper=full, lower=simple))
summary(forwmodel)
AIC(forwmodel)
library(leaps)
bestss <- regsubsets(logmodel, method="forward")
bestss <- regsubsets(logmodel, method="forward", data=credit_clean)
bestss <- regsubsets(logmodel, y=credit_clean[,response], method="forward")
bestss <- regsubsets(full, y=credit_clean[,response], method="forward")
simple<-glm(default_payment_next_month ~ 1,family=binomial(link='logit'), data=train)
full<-glm(default_payment_next_month ~.,family=binomial(link='logit'), data=train)
forwmodel<-step(simple, direction="forward", scope=list(upper=full, lower=simple))
summary(forwmodel)
forwmodel$aic
backmodel<-backwards(response, var, data=train)
summary(backmodel)
back_aic<-backmodel$aic
for_aic<-forwmodel$aic
backmodel2<-step(full, method="backward", scope=list(upper=full, lower=simple))
summary(backmodel2)
back2_aic<-backmodel2$aic
warnings()
simple<-glm(default_payment_next_month ~ 1,family=binomial(link='logit'), data=train)
full<-glm(default_payment_next_month ~.,family=binomial(link='logit'), data=train)
forwmodel<-step(simple, direction="forward", scope=list(upper=full, lower=simple))
summary(forwmodel)
#setwd("~/Trabajos Carlota/ERASMUS/IIT/CS422-Data Mining/Homework/hw4")
ratings<-read.csv('~/Trabajos Carlota/ERASMUS/IIT/CS422-Data Mining/Homework/hw4/ratings.csv', sep=',', header=T, stringsAsFactors = F)
movies<-read.csv('~/Trabajos Carlota/ERASMUS/IIT/CS422-Data Mining/Homework/hw4/movies.csv', sep=',', header=T, stringsAsFactors = F)
path<-'~/Trabajos Carlota/ERASMUS/IIT/CS422-Data Mining/Homework/hw4/hw4.movies'
if(dir.exists(path)==FALSE){dir.create(path)}
files<-list.files(path, full.names = T)
#We use sql queries to obtain the information and import it into a file
library('sqldf')
sqlStr<-'SELECT ratings.userId, movies.title
FROM ratings INNER JOIN movies ON ratings.movieId==movies.movieId'
moviesAndRatings<-sqldf(sqlStr)
#We are going to see how many unique users we have:
sqlStr<-'SELECT DISTINCT userId FROM moviesAndRatings'
users<-sqldf(sqlStr)
#We do this so that the files are only created once
if(sum(file.exists(files))!=length(users$userId)){
for(i in 1:length(users$userId)){
setwd(path) #We change the directory so that all the files are created there
index<-which(moviesAndRatings$userId==users$userId[i])
name<-paste0('user', i, '.txt')
write(moviesAndRatings$title[index], file=name, sep='\t')
}
files<-list.files(path, full.names = T)
}
library(textreuse)
corpus <- TextReuseCorpus(files, tokenizer = tokenize_ngrams, n = 5, keep_tokens = TRUE)
col<-nrow(users)
shingles<-tokens(corpus)
#row<-sum(wordcount(corpus)) #it tells us duplicates
shingles_clean<-unique(shingles)
names(shingles_clean)<-names(shingles)
row<-0
for(i in 1:length(users$userId)){
row<-row+length(shingles_clean[[i]])
}
cat('There are' , col, 'columns and ' , row, 'rows')
#TODO: characteristic matrix
A<matrix(0,nrow=row, ncol=col)
A<-matrix(0,nrow=row, ncol=col)
View(A)
head(A)
sort(names(shingles))
names(shingles)
colnames(A)<-names(shingles)
head(A)
shingles_clean[1]
shingles_clean[1]=="user1"
b<-head(shingles_clean)
b<-as.data.frame(b)
labels(shingles)
b
b[1]
name(b[1])
names(b[1])
names(b[1])=="user1"
which(shingles_clean==b[[1]])
which(shingles_clean[[1]]==b[[1]])
index<-1:671
which(shingles_clean[[index]]==b[[1]])
user20 <- corpus[['user20']]
length(user20)
user20
user20 <- corpus[["user20"]]
length(user20)
library(stringr)
str_count(user20,'\t')
str_count(user20,'\n')
library(stringr)
user20 <- corpus[["user20"]]
n<-str_count(user20, '\n')+1 #We add one to take into consideration the last movie of the list
cat('The number of movies that user20 rated was: ', n)
tokens(user20)[1:5]
wordcount(corpus)
col<-nrow(users)
shingles<-tokens(corpus)
#row<-sum(wordcount(corpus)) #it tells us duplicates
shingles_clean<-unique(shingles)
names(shingles_clean)<-names(shingles)
row<-0
for(i in 1:length(users$userId)){
row<-row+length(shingles_clean[[i]])
}
cat('There are' , col, 'columns and ' , row, 'rows')
setwd("C:/Users/Carru/SoftwareRepositories/project571/data model")
data<-read.csv('../data collection/Data/ModelData/modelData.csv', header=T, sep=";", na.strings='Null')
#We start observing the data
index<-which(complete.cases(data)==FALSE)
#data[index, ]
#There are 3 accounts that stopped tweeting for a period of time/stopped functioning
#TODO: discuss what to do with these accounts
#Create variable isWeekend:
data[, "date"]<-as.Date(data[, "date"])
library('lubridate')
day<-wday(data[,"date"], week_start=1) #starts on Monday
data[, "isWeekend"]<-0
data$isWeekend[which(day>=5)]<-1 #We consider that the weekend starts on Friday
#Create variable change in number of followers
#TODO: remove the line that gets the number of followers for 23/02 when we put these values in the
#modelData.csv
followers<- read.csv('../data collection/Data/ModelData/historicFollowers.csv',header = T,stringsAsFactors = F,sep = ';',na.strings = 'Null')
data[, "change_followers"]<-0
data$change_followers[which(data$date=="2018-02-24")]<-data$Followers[which(data$date=="2018-02-24")]-followers$X2018.02.23
#TODO: change this when we have all the data
startday <-  data$date[1]
endday <- data$date[nrow(data)]
a1 <- data$Followers[which((data$date> (startday-1)) & (data$date< endday))]
b1 <- data$Followers[which((data$date> startday) & (data$date<(endday+1)))]
data$change_followers[data$date>startday] <- b1 - a1
#We create a category column
account<-read.csv('../data collection/Data/ModelData/accountsComplete.csv', header = T,stringsAsFactors = F,sep = ';',na.strings = 'Null')
data[, "category"]<-"a"
data$category[which(data$Account==account$Account)]<-account$Category
data$category<-as.factor(data$category)
#We remove the Accounts that have NAs
i<-which((data$Account=="sportbible")|(data$Account=="fabulousanimals")|(data$Account=="Earth_Pics"))
cat('The percentage of removed data is:', (length(i)/dim(data)[1])*100) #So we can delete them
data_clean<-data[-i, ]
#-----------------------------------------------------------------------------------------------
#Correlation between variables
type<-function(a, funct){
if(sum(sapply(a, funct))==0){
return(0)
}else{
return(names(which(sapply(a, funct))))
}
}
numVar<-type(data_clean, is.numeric)
catVar<-type(data_clean, is.factor)
dateVar<-type(data_clean, is.Date)
stopifnot((length(numVar)+length(catVar)+length(dateVar))==ncol(data_clean))
library('corrplot')
corMatrix <- cor(data_clean[, numVar])
corrplot(corMatrix, method = 'number', diag = TRUE)
#From this matrix we see that the values show no relevant linear correlation except for pRTs and
#pMentions and Followers with change_followers
#We are going to create a variable 1-Increase 0-Decrease or stayed the same
data_clean[, "logic_change"]<-0
data_clean$logic_change[which(data_clean$change_followers>0)]<-1
#We use stratified sampling to divide the data into training and test
library('lattice')
library('ggplot2')
library('caret')
set.seed(1234)
ind<-createDataPartition(y=data_clean$logic_change, list=FALSE, p=0.8)
train<-data_clean[ind,]
test<-data_clean[-ind,]
stopifnot(nrow(train) + nrow(test) == nrow(data_clean))
createModelFormula <- function(targetVar, xVars, includeIntercept = TRUE){
if(includeIntercept){
modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ ')))
} else {
modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ '), -1))
}
return(modelForm)
}
#TODO: Remove highly correlated variables?
xVars<-numVar[-(which(numVar=="change_followers"))]
xVars<-xVars[-(which(numVar=="Followers"))] #We remove Followers because it was highly correlated
xVars<-c(xVars, "category")
modelForm <- createModelFormula(targetVar = "logic_change", xVars = xVars, includeIntercept = TRUE)
logmodel<-glm(modelForm, family=binomial(link='logit'), data=train)
summary(logmodel)
logpred<-predict(logmodel, test, type="response")
plot(logpred)
x<-sort(logpred)
head(x)
tail(x)
plot(x)
xVars<-numVar[-(which(numVar=="change_followers"))]
#xVars<-xVars[-(which(numVar=="Followers"))] #We remove Followers because it was highly correlated
xVars<-c(xVars, "category")
modelForm <- createModelFormula(targetVar = "logic_change", xVars = xVars, includeIntercept = TRUE)
logmodel<-glm(modelForm, family=binomial(link='logit'), data=train)
summary(logmodel)
logpred<-predict(logmodel, test, type="response")
x<-sort(logpred)
plot(x)
xVars<-numVar[-(which(numVar=="change_followers"))]
xVars<-xVars[-(which(numVar=="Followers"))] #We remove Followers because it was highly correlated
xVars<-c(xVars, "category")
modelForm <- createModelFormula(targetVar = "logic_change", xVars = xVars, includeIntercept = TRUE)
logmodel<-glm(modelForm, family=binomial(link='logit'), data=train)
summary(logmodel)
logpred<-predict(logmodel, test, type="response")
threshold<-0.5
defaulted<-rep(0, length(test$logic_change))
defaulted[logpred>threshold]<-1
defaulted<-as.logical(defaulted)
table(defaulted, test$logic_change)
mean(test$logic_change)
#By default it has a 86% of probability of increasing
mean(defaulted==test$logic_change)
sum(defaulted)
sum(defaulted==TRUE)
#By default it has a 86% of probability of increasing
confusionMatrix(defaulted, test)
#By default it has a 86% of probability of increasing
confusionMatrix(defaulted, test$logic_change)
str(test$logic_change)
threshold<-0.5
defaulted<-rep(0, length(test$logic_change))
defaulted[logpred>threshold]<-1
#defaulted<-as.logical(defaulted)
table(defaulted, test$logic_change)
#By default it has a 86% of probability of increasing
confusionMatrix(defaulted, test$logic_change)
ggplot(x)
x<-sort(logpred)
plot(x, ylab='Probability', main='Logistic regression, full model')
logpred<-predict(logmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$logic_change))
defaulted[logpred>threshold]<-1
mean(test$logic_change)
#86% of our data corresponds to an increase of the number of followers, so there is a clear bias
#in our data
confusionMatrix(defaulted, test$logic_change)
#Our model has 86,% of probability of correctly saying whether it's going to increase or not
#so, it's not better than the default setting.
#Since there is a clear bias in our data set, there is a huge difference between Specificity &
#Sensitivity
x<-sort(logpred)
plot(x, ylab='Probability', main='Logistic regression, full model')
x<-sort(logpred)
plot(x, ylab='Probability', main='Logistic regression, full model')
defaulted<-as.logical(defaulted)
x<-sort(logpred)
plot(x, ylab='Probability', main='Logistic regression, full model')
plot(x)
head(x)
xVars<-numVar[-(which(numVar=="change_followers"))]
#xVars<-xVars[-(which(numVar=="Followers"))] #We remove Followers because it was highly correlated
xVars<-c(xVars, "category")
modelForm <- createModelFormula(targetVar = "logic_change", xVars = xVars, includeIntercept = TRUE)
logmodel<-glm(modelForm, family=binomial(link='logit'), data=train)
summary(logmodel)
logpred<-predict(logmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$logic_change))
defaulted[logpred>threshold]<-1
mean(test$logic_change)
#86% of our data corresponds to an increase of the number of followers, so there is a clear bias
#in our data
confusionMatrix(defaulted, test$logic_change)
#Our model has 86,% of probability of correctly saying whether it's going to increase or not
#so, it's not better than the default setting.
#Since there is a clear bias in our data set, there is a huge difference between Specificity &
#Sensitivity
x<-sort(logpred)
plot(x, ylab='Probability', main='Logistic regression, full model')
xVars<-numVar[-(which(numVar=="change_followers"))]
xVars<-xVars[-(which(numVar=="Followers"))] #We remove Followers because it was highly correlated
xVars<-c(xVars, "category")
modelForm <- createModelFormula(targetVar = "logic_change", xVars = xVars, includeIntercept = TRUE)
logmodel<-glm(modelForm, family=binomial(link='logit'), data=train)
summary(logmodel)
logpred<-predict(logmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$logic_change))
defaulted[logpred>threshold]<-1
mean(test$logic_change)
#86% of our data corresponds to an increase of the number of followers, so there is a clear bias
#in our data
confusionMatrix(defaulted, test$logic_change)
#Our model has 86,% of probability of correctly saying whether it's going to increase or not
#so, it's not better than the default setting.
#Since there is a clear bias in our data set, there is a huge difference between Specificity &
#Sensitivity
x<-sort(logpred)
plot(x, ylab='Probability', main='Logistic regression, full model')
simple<-glm(logic_change ~ 1,family=binomial(link='logit'), data=train)
full<-glm(logic_change ~.,family=binomial(link='logit'), data=train)
logmodel2<-step(simple, direction="both", scope=list(upper=full, lower=simple))
warnings()
names(xVars)
names(xVar)
xVars
names(data_clean)
simple<-glm(logic_change ~ 1,family=binomial(link='logit'), data=train)
full<-glm(modelForm,family=binomial(link='logit'), data=train)
logmodel2<-step(simple, direction="both", scope=list(upper=full, lower=simple))
summary(logmodel2)
logmodel2$coefficients
logmodel2$linear.predictors
logmodel2$formula
xVars
summary(logmodel2)
logpred2<-predict(logmodel2, test, type="response")
threshold<-0.5
defaulted2<-rep(0, length(test$logic_change))
defaulted2[logpred2>threshold]<-1
confusionMatrix(defaulted2, test$logic_change)
confusionMatrix(defaulted, test$logic_change)
summary(logmodel)
a<-step(full, direction="backward", scope=list(upper=full, lower=simple))
summary(a)
logmodel2$aic
rm(a)
x2<-sort(logpred2)
plot(x2)
modelForm <- createModelFormula(targetVar = "logic_change", xVars = xVars, includeIntercept = FALSE)
logmodel<-glm(modelForm, family=binomial(link='logit'), data=train)
summary(logmodel)
#TODO: once all the variables are set, analyze the coefficients
logpred<-predict(logmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$logic_change))
defaulted[logpred>threshold]<-1
mean(test$logic_change)
#86% of our data corresponds to an increase of the number of followers, so there is a clear
#class imbalance
confusionMatrix(defaulted, test$logic_change)
response<-"logic_change"
xVars
library(Matrix)
library(foreach)
library(glmnet)
x<-model.matrix(~ nTweets + pHashtags + pMentions + pURLs + pMedia + pRTs + isWeekend + category -1, train)
y<-as.matrix.data.frame(train[, response])
#We use cross-validation to choose the tuning parameter (lambda)
cv.lasso<-cv.glmnet(x, y, alpha=1)
best.lasso<-cv.lasso$lambda.min #we get a lambda of 0.0024
xtest<-model.matrix(~ nTweets + pHashtags + pMentions + pURLs + pMedia + pRTs + isWeekend + category -1, test)
lasso_mod<-glmnet(x, y, alpha=1, lambda=c(1,10,1))
lasso.pre<-predict(lasso_mod, s=best.lasso, xtest)
y<-as.matrix.data.frame(train[, response])
y<-as.matrix.data.frame(train$logic_change)
y<-as.matrix(train$logic_change)
cv.lasso<-cv.glmnet(x, y, alpha=1)
best.lasso<-cv.lasso$lambda.min #we get a lambda of 0.0024
xtest<-model.matrix(~ nTweets + pHashtags + pMentions + pURLs + pMedia + pRTs + isWeekend + category -1, test)
lasso_mod<-glmnet(x, y, alpha=1, lambda=c(1,10,1))
lasso.pre<-predict(lasso_mod, s=best.lasso, xtest)
which(lasso.pred>0.5)
which(lasso.pre>0.5)
sum(lasso.pre>0.5)
min(lasso.pre)
lasso.coef<-predict(lasso_mod, type="coefficients", s=best.lasso)
lasso.coef
inTrain <- createDataPartition(y = data_clean[,targetVar], list = FALSE, p = 0.8)
train <- data_clean[inTrain,]
test <- data_clean[-inTrain,]
stopifnot(nrow(train) + nrow(test) == nrow(data_clean))
sum(train$change_followers)/nrow(train)
sum(test$change_followers)/nrow(test)
modelForm <- createModelFormula(targetVar = targetVar, xVars = xVars, includeIntercept = TRUE)
model <- lm(modelForm, data = train)
summary(model)
inTrain <- createDataPartition(y = data_clean[,targetVar], list = FALSE, p = 0.8)
train <- data_clean[inTrain,]
test <- data_clean[-inTrain,]
stopifnot(nrow(train) + nrow(test) == nrow(data_clean))
sum(train$change_followers)/nrow(train)
sum(test$change_followers)/nrow(test)
targetVar <-  "change_followers"
inTrain <- createDataPartition(y = data_clean[,targetVar], list = FALSE, p = 0.8)
train <- data_clean[inTrain,]
test <- data_clean[-inTrain,]
stopifnot(nrow(train) + nrow(test) == nrow(data_clean))
sum(train$change_followers)/nrow(train)
sum(test$change_followers)/nrow(test)
modelForm <- createModelFormula(targetVar = targetVar, xVars = xVars, includeIntercept = TRUE)
model <- lm(modelForm, data = train)
summary(model)
model$df.residual
summary(model)[2]
summary(model)[5]
summary(model)[10]
labels(summary(model))
summary(model)$r.squared
summary(model)$adj.r.squared
pred<-predict(model, test)
pred<-as.data.frame(pred)
pred[, "actual"]<-test[, "change_followers"]
RSE<-sum((pred[,"actual"]-pred[, "pred"])**2)
ymean<-mean(test[, "change_followers"])
Rtot<-sum((test[, "change_followers"]-ymean)**2)
R_squared<-1-(RSE/Rtot)
cat("The value of the R squared for the test is", R_squared)
summary(model)
cat('The adjusted r squared is: ', adj.r.squared)
xVars<-xVars[-(which(numVar=="nTweets"))]
modelForm <- createModelFormula(targetVar = targetVar, xVars = xVars, includeIntercept = TRUE)
model2 <- lm(modelForm, data = train)
summary(model2)
pred2<-predict(model, test)
pred2<-as.data.frame(pred2)
pred2[, "actual"]<-test[, "change_followers"]
RSE2<-sum((pred2[,"actual"]-pred2[, "pred"])**2)
ymean<-mean(test[, "change_followers"])
Rtot<-sum((test[, "change_followers"]-ymean)**2)
R_squared2<-1-(RSE2/Rtot)
cat("The value of the R squared for the test is", R_squared2)
pred2<-predict(model2, test)
pred2<-as.data.frame(pred2)
pred2[, "actual"]<-test[, "change_followers"]
RSE2<-sum((pred2[,"actual"]-pred2[, "pred"])**2)
RSE2<-sum((pred2[,"actual"]-pred2[, "pred2"])**2)
ymean<-mean(test[, "change_followers"])
Rtot<-sum((test[, "change_followers"]-ymean)**2)
R_squared2<-1-(RSE2/Rtot)
cat("The value of the R squared for the test is", R_squared2)
RsquaredLM<-function(pred, test){
pred<-as.data.frame(pred)
pred[, "actual"]<-test[, "change_followers"]
RSE<-sum((pred[,"actual"]-pred[, "pred"])**2)
ymean<-mean(test[, "change_followers"])
Rtot<-sum((test[, "change_followers"]-ymean)**2)
R_squared<-1-(RSE/Rtot)
}
R<-RsquaredLM(pred, test)
R<-RsquaredLM(pred2, test)
as.character(names(pred))
RsquaredLM<-function(pred, test){
pred<-as.data.frame(pred)
n<-as.character(names(pred))
pred[, "actual"]<-test[, targetVar]
RSE<-sum((pred[,"actual"]-pred[, n])**2)
ymean<-mean(test[, targetVar])
Rtot<-sum((test[, targetVar]-ymean)**2)
R_squared<-1-(RSE/Rtot)
}
R<-RsquaredLM(pred, test)
R<-RsquaredLM(pred2, test)
rm(R)
RsquaredLM<-function(pred, test, targetVar){
pred<-as.data.frame(pred)
n<-as.character(names(pred))
pred[, "actual"]<-test[, targetVar]
RSE<-sum((pred[,"actual"]-pred[, n])**2)
ymean<-mean(test[, targetVar])
Rtot<-sum((test[, targetVar]-ymean)**2)
R_squared<-1-(RSE/Rtot)
}
R<-RsquaredLM(pred, test, targetVar)
R<-RsquaredLM(pred2, test, targetVar)
simple_lm<-lm(change_followers ~ 1,family=binomial(link='logit'), data=train)
full_lm<-glm(modelForm,family=binomial(link='logit'), data=train)
lmStep<-step(simple, direction="both", scope=list(upper=full_lm, lower=simple_lm))
summary(lmStep)
simple_lm<-lm(change_followers ~ 1, data=train)
full_lm<-glm(modelForm, data=train)
lmStep<-step(simple_lm, direction="both", scope=list(upper=full_lm, lower=simple_lm))
summary(lmStep)
summary(model)$adj.r.squared
summary(model2)$adj.r.squared
cat('The adjusted r squared is: ', summary(lmStep)$adj.r.squared)
lmStep$formula
labels(summary(lmStep))
summary(lmStep)$call
xVars
b<-xVars[-which(xVars=="pRTs")]
m<-createModelFormula(targetVar, b, includeIntercept = TRUE)
simple_lm<-lm(change_followers ~ 1, data=train)
full_lm<-lm(modelForm, data=train)
lmStep<-step(simple_lm, direction="both", scope=list(upper=full_lm, lower=simple_lm))
summary(lmStep)
cat('The adjusted r squared is: ', summary(lmStep)$adj.r.squared)
x
b
m_noTweetsRTs<-lm(m,train)
summary(m_noTweetsRTs)
a<-predict(m_noTweetsRTs, test)
R<-RsquaredLM(a, test, targetVar)
