i<-which((data$Account=="sportbible")|(data$Account=="fabulousanimals")|(data$Account=="Earth_Pics"))
cat('The percentage of removed data is:', (length(i)/dim(data)[1])*100, '\n') #So we can delete them
data_clean<-data[-i, ]
#-----------------------------------------------------------------------------------------------
#Correlation between variables
type<-function(a, funct){
if(sum(sapply(a, funct))==0){
return(0)
}else{
return(names(which(sapply(a, funct))))
}
}
numVar<-type(data_clean, is.numeric)
catVar<-type(data_clean, is.factor)
dateVar<-type(data_clean, is.Date)
stopifnot((length(numVar)+length(catVar)+length(dateVar))==ncol(data_clean))
library('corrplot')
corMatrix <- cor(data_clean[, numVar])
corrplot(corMatrix, method = 'number', diag = TRUE)
#From this matrix we see that the values show no relevant linear correlation except for pRTs and
#pMentions and Followers with change_followers
#TODO: Modify this in the models
#-----------------------------------------------------------------------------------------------
#MODELS
#-----------------------------------------------------------------------------------------------
#Logistic Regression Model
#We are going to create a variable 1-Increase 0-Decrease or stayed the same
data_clean[, "logic_change"]<-0
data_clean$logic_change[which(data_clean$change_followers>0)]<-1
#We use stratified sampling to divide the data into training and test
library('lattice')
library('ggplot2')
library('caret')
set.seed(1234)
ind<-createDataPartition(y=data_clean$logic_change, list=FALSE, p=0.8)
train<-data_clean[ind,]
test<-data_clean[-ind,]
stopifnot(nrow(train) + nrow(test) == nrow(data_clean))
createModelFormula <- function(targetVar, xVars, includeIntercept = TRUE){
if(includeIntercept){
modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ ')))
} else {
modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ '), -1))
}
return(modelForm)
}
#TODO: Remove highly correlated variables?
#TODO: Introduce the groups replacing the followers
xVars<-numVar[-(which(numVar=="change_followers"))]
xVars<-xVars[-(which(numVar=="Followers"))] #We remove Followers because it was highly correlated
xVars<-c(xVars, "category")
response<-"logic_change"
modelForm <- createModelFormula(targetVar = response, xVars = xVars, includeIntercept = TRUE)
logmodel<-glm(modelForm, family=binomial(link='logit'), data=train)
summary(logmodel)
#TODO: once all the variables are set, analyze the coefficients
logpred<-predict(logmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$logic_change))
defaulted[logpred>threshold]<-1
mean(test$logic_change)
#86% of our data corresponds to an increase of the number of followers, so there is a clear
#class imbalance
confusionMatrix(defaulted, test$logic_change)
#Our model has 86,% of probability of correctly saying whether it's going to increase or not
#so, it's not better than the default setting.
#Since there is a clear class imbalance in our data set, there is a huge difference
#between Specificity & Sensitivity
x<-sort(logpred)
plot(x, ylab='Probability', main='Logistic regression, full model')
#Since we barely have data for the lower part of the sigmoid curve, we can't draw it properly.
#Now we are going to proceed by running Stepwise selection to see if we obtain a better model
#Stepwise Selection
library(MASS)
library(stats)
simple<-glm(logic_change ~ 1,family=binomial(link='logit'), data=train)
full<-glm(modelForm,family=binomial(link='logit'), data=train)
logmodel2<-step(simple, direction="both", scope=list(upper=full, lower=simple))
summary(logmodel2)
#logmodel2$formula
#Applying stepwise selection we get that the best model is
#logic_change ~ category + pMentions + pURLs + pMedia + nTweets + isWeekend
#This makes sense as pRTs was highly correlated to pMentions
#Hashtags are simply not important
#Prediction:
logpred2<-predict(logmodel2, test, type="response")
threshold<-0.5
defaulted2<-rep(0, length(test$logic_change))
defaulted2[logpred2>threshold]<-1
confusionMatrix(defaulted2, test$logic_change)
#No changes between both models, we get the same results
#No change in the graph either
# #Lasso Regression:
# library(Matrix)
# library(foreach)
# library(glmnet)
#
# x<-model.matrix(~ nTweets + pHashtags + pMentions + pURLs + pMedia + pRTs + isWeekend + category -1, train)
# y<-as.matrix(train$logic_change)
# #We use cross-validation to choose the tuning parameter (lambda)
# cv.lasso<-cv.glmnet(x, y, alpha=1)
# best.lasso<-cv.lasso$lambda.min #we get a lambda of 0.00024
#
# xtest<-model.matrix(~ nTweets + pHashtags + pMentions + pURLs + pMedia + pRTs + isWeekend + category -1, test)
# lasso_mod<-glmnet(x, y, alpha=1, lambda=c(1,10,1))
# lasso.pre<-predict(lasso_mod, s=best.lasso, xtest)
#-------------
#Regression Model
#TODO: EXPLANATION OF THE COEFFICIENTS
#TODO: Remove highly correlated variables?
#xVars<-c(xVars, "Followers")
targetVar <-  "change_followers"
#We have to partition the data again, since now we have a different targetVar
inTrain <- createDataPartition(y = data_clean[,targetVar], list = FALSE, p = 0.8)
train <- data_clean[inTrain,]
test <- data_clean[-inTrain,]
stopifnot(nrow(train) + nrow(test) == nrow(data_clean))
sum(train$change_followers)/nrow(train)
sum(test$change_followers)/nrow(test)
modelForm <- createModelFormula(targetVar = targetVar, xVars = xVars, includeIntercept = TRUE)
model <- lm(modelForm, data = train)
summary(model)
#If we use Followers we have a R_squared of 0.54 for both the training and the test
#If we don't use it, we have a R_squared of 0.2234
cat('The adjusted r squared is: ', summary(model)$adj.r.squared)
#Prediction:
pred<-predict(model, test)
#The following function calculates the R_squared of the prediction
RsquaredLM<-function(pred, test, targetVar){
pred<-as.data.frame(pred)
n<-as.character(names(pred))
pred[, "actual"]<-test[, targetVar]
RSE<-sum((pred[,"actual"]-pred[, n])**2)
ymean<-mean(test[, targetVar])
Rtot<-sum((test[, targetVar]-ymean)**2)
R_squared<-1-(RSE/Rtot)
}
R_squared<-RsquaredLM(pred, test, targetVar)
cat("The value of the R squared for the test is", R_squared)
xVars<-xVars[-(which(numVar=="nTweets"))]
modelForm <- createModelFormula(targetVar = targetVar, xVars = xVars, includeIntercept = TRUE)
model2 <- lm(modelForm, data = train)
summary(model2)
cat('The adjusted r squared is: ', summary(model2)$adj.r.squared) #0.2236
pred2<-predict(model2, test)
R_squared2<-RsquaredLM(pred2, test, targetVar)
cat("The value of the R squared for the test is", R_squared2)
#We get the same R squared as before
#plot(model2)
#TODO: EXPLANATION OF THE PLOTS
#Stepwise Selection
simple_lm<-lm(change_followers ~ 1, data=train)
full_lm<-lm(modelForm, data=train)
lmStep<-step(simple_lm, direction="both", scope=list(upper=full_lm, lower=simple_lm))
summary(lmStep)
cat('The adjusted r squared is: ', summary(lmStep)$adj.r.squared) #0.2236
#We use cross-validation to choose the optimal value of lambda
cv.ridge <-cv.glmnet (x, y ,alpha =0)
# 1. Please write a function called backwards() that implements the
# backward selection algorithm using AIC.
backwards<-function(response, var, data){
#response is the response variable and var is an array with the names of the explanatory variables
f<-as.formula(paste(response, "~", paste(var, collapse='+ ')))
model1<-glm(f, family=binomial(link='logit'), data=data)
A1<-AIC(model1)
new_AIC<-rep(A1, length(var))
for(i in 1:length(var)){
new_var<-var[-i]
f<-as.formula(paste(response, "~", paste(new_var, collapse='+ ')))
m<-glm(f, family=binomial(link='logit'), data=data)
new_AIC[i]<-AIC(m)
}
index<-which(new_AIC==min(new_AIC))
if(new_AIC[index]<A1){
#We do this in case the best solution is the one with the intercept
if(grepl(new_AIC[index], "(Intercept)")){
return(model1)
}else{
cat('Removed variable: ', as.character(var[index]))
cat('\n')
backwards(response, var[-index], data)
}
}else{
return(model1)
}
}
# 2. Download the credit card default data set from the UCI machine learning
# repository. https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients
# Load the data into R.
library(readxl)
path<-'~/Trabajos Carlota/ERASMUS/IIT/CSP-MATH 571 Data Preparation and Analysis/homework/homework4/defaultofcreditcardclients.xls'
credit<-read_excel(path, col_names=T, skip=1)
credit<-credit[, -1] #we drop the ID column
names(credit)<-tolower(names(credit))
colnames(credit)[colnames(credit)=="default payment next month"]<-"default_payment_next_month"
str(credit) #we check the data type of each attribute and we change them accordingly
credit<-within(credit, {
sex<-factor(sex, levels=c(1,2), labels=c("male", "female"))
education<-factor(education, levels=c(1,2,3,4), labels=c("graduate school", "university", "high school", "others"))
marriage<-factor(marriage, levels=c(1,2,3), labels=c("married", "single", "others"))
default_payment_next_month<-as.logical(default_payment_next_month)
})
#TODO: change the response to factor?
#TODO: ask about -2 and 0. Are they NAs?
#TODO: label pay_0.... ?
names<-c("pay_0", "pay_2", "pay_3", "pay_4", "pay_5", "pay_6")
b<-sapply(credit[, names], as.factor)
credit[, names]<-as.data.frame(b)
rm(b)
# 3. Identify all the relevant categorical, numeric, and logical variables.
type<-function(a, funct){
if(sum(sapply(a, funct))==0){
return(0)
}else{
return(names(which(sapply(a, funct))))
}
}
#TOOD: depending on what I say default_payment_next_month is, add it or not
catVars<-type(credit, is.factor)
numVars<-type(credit, is.numeric)
logVars<-type(credit, is.logical)
# 4. Perform all required EDA on this data set.
sum(complete.cases((credit)==FALSE))
summary(credit)
#From this we can see that the attributes educations and marriage have NAs.
#Since the number of Na's is low (1.3%) compared to the dataset we can eliminate them, since there
#are not two attributes that you could guess easily
index<-which(complete.cases(credit)==FALSE)
credit_clean<-credit[-index, ]
#We can also see that pay_0-pay_6 all have "0" and "-2" which is a value that is not defined in
#the description of the data. The number of times that these values have been introduced in the
#dataframe is too high for it to be considered as random. Since the error is consistent it must
#have a meaning, but since we can't ask the person who introduced the values, we assume that:
#TODO: what to do which 0/-2
sum(credit$default_payment_next_month)/nrow(credit)
#We can see that there's not an even distribution of the target variable which means that there
#is not a lot of information to create a good estimate for the "yes" case.
sum(credit_clean$default_payment_next_month)/nrow(credit_clean)
#The percentage of "yes" is the same as before
library('corrplot')
corMatrix <- cor(credit_clean[, numVars])
corrplot(corMatrix, method = 'number', diag = TRUE)
#From this we can see that there is a high correlation between bill_amt1 and the rest of the
#bill_amt factors. The rest of the variables have low correlation with the rest
# 5.Build a logistic regression model to determine whether or not a
# customer defaulted. Use all of the variables. Validate the model on a
# test data set. Use the comments to discuss the performance of the model.
response<-"default_payment_next_month"
#We do stratified sampling to separate the model
library('lattice')
library('ggplot2')
library('caret')
set.seed(1122)
ind<-createDataPartition(y=credit_clean$default_payment_next_month, list=FALSE, p=0.8)
train<-credit_clean[ind,]
test<-credit_clean[-ind,]
stopifnot(nrow(train) + nrow(test) == nrow(credit_clean))
sum(train$default_payment_next_month)/nrow(train)
sum(test$default_payment_next_month)/nrow(test)
#The percentage of the response="yes" stays the same
var<-names(credit_clean)
var<-var[-which(var==response)] #We are asked to use all variables
f<-as.formula((paste(response, "~", paste(var, collapse='+ '))))
logmodel<-glm(f, family=binomial(link='logit'), data=train)
summary(logmodel)
#TODO: REMOVE THIS
#logmodel$xlevels[["pay_5"]]<-union(logmodel$xlevels[["pay_5"]], levels(test$pay_5))
#logmodel$xlevels[["pay_4"]]<-union(logmodel$xlevels[["pay_4"]], levels(test$pay_4))
logpred<-predict(logmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[logpred>threshold]<-1
defaulted<-as.logical(defaulted)
table(defaulted, test$default_payment_next_month)
#We can see in this table the distribution of how our model predicted the outcome
mean(defaulted)
#we obtained that only 12% of the cases would we yes, when we know that the percentage of "yes"
#in the data is 22%
mean(test$default_payment_next_month)
#The mean of "yes" is in both cases really small, since there are low number of examples that
#has 1 as an outcome.
confusionMatrix(defaulted, test$default_payment_next_month)
#The performance of the model is quite good as it has an accuracy of 0.82 which is higher than
#the No information Rate, so we can say that the model is good.
#We have high recall and precision
log_aic<-AIC(logmodel)
log.mse<-mean((test[,response]-defaulted)**2)
# 6. Using forward selection, determine the best model.
#I'm going to use AIC to decide which is the best model
library(MASS)
library(stats)
simple<-glm(default_payment_next_month ~ 1,family=binomial(link='logit'), data=train)
full<-glm(default_payment_next_month ~.,family=binomial(link='logit'), data=train)
forwmodel<-step(simple, direction="forward", scope=list(upper=full, lower=simple))
summary(forwmodel)
for_aic<-forwmodel$aic #The aic is smaller than before
#In this case, the best model is:
#default_payment_next_month ~ pay_0 + pay_4 + limit_bal + pay_6 + pay_3 + pay_amt1 + bill_amt2 +
#marriage + pay_amt2 + education + sex + bill_amt3 + pay_5 + pay_amt5 + pay_amt6 + age
#Prediction
forpred<-predict(forwmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[forpred>threshold]<-1
defaulted<-as.logical(defaulted)
mean(defaulted) #We get that the mean in the defaulted is 12%
confusionMatrix(defaulted, test$default_payment_next_month)
#The accuracy is slightly smaller but it is still around 82% and the recall and
#precision still have a value of 95% and 84%. So there seems to be no improvement
forw.mse<-mean((test[,response]-defaulted)**2)
#The error is slightly higher than in the previous case
# 7. Using the backwards selection function you implemented in #1
# , determine the best model.
backmodel<-backwards(response, var, data=train)
summary(backmodel)
back_aic<-backmodel$aic #The aic is slightly smaller than before
#default_payment_next_month ~ pay_0 + pay_4 + limit_bal + pay_6 + pay_3 + pay_amt1 +
#marriage + pay_amt2 + education + sex + bill_amt3 + pay_5 + pay_amt5 + pay_amt6 + age
#Prediction
backpred<-predict(backmodel, test, type="response")
#Now we set a parameter to measure what we consider as a "Yes" and what we consider as "No"
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[backpred>threshold]<-1
defaulted<-as.logical(defaulted)
mean(defaulted) #We get that the mean in the defaulted is 12%
confusionMatrix(defaulted, test$default_payment_next_month)
#The accuracy is slightly smaller but it is still around 82% and the recall and
#precision still have a value of 95% and 84%. So there seems to be no improvement
back.mse<-mean((test[,response]-defaulted)**2)
#The error is smaller than in the forward selection case but still slightly higher than in the
#case where we had all the variables
# 8. Run an implementation of backwards selection found in an R package on this
# data set. Discuss any differences between the results of this implementation
# and your implementation in question 7.
backmodel2<-step(full, method="backward", scope=list(upper=full, lower=simple))
summary(backmodel2)
back_aic2<-backmodel2$aic
#In both cases we get the same model, with the same value of aic and
#same value for the coefficients. The only difference between both is that
#this function takes less time to compute
# 9. Run lasso regression on the data set. Briefly discuss how you determined
# the appropriate tuning parameters.
library(Matrix)
library(foreach)
library(glmnet)
#this function standardizes the values
x<-model.matrix(~limit_bal+ sex+ education+ marriage+ age+ pay_0+ pay_2+ pay_3+ pay_4+ pay_5+ pay_6+ bill_amt1+ bill_amt2+ bill_amt3+ bill_amt4+ bill_amt5+ bill_amt6+ pay_amt1+ pay_amt2+ pay_amt3+ pay_amt4+ pay_amt5+ pay_amt6 -1, train)
y<-as.matrix.data.frame(train[, response])
#We use cross-validation to choose the tuning parameter (lambda)
cv.lasso<-cv.glmnet(x, y, alpha=1)
best.lasso<-cv.lasso$lambda.min #we get a lambda of 0.0024
xtest<-model.matrix(~limit_bal+ sex+ education+ marriage+ age+ pay_0+ pay_2+ pay_3+ pay_4+ pay_5+ pay_6+ bill_amt1+ bill_amt2+ bill_amt3+ bill_amt4+ bill_amt5+ bill_amt6+ pay_amt1+ pay_amt2+ pay_amt3+ pay_amt4+ pay_amt5+ pay_amt6 -1, test)
lasso_mod<-glmnet(x, y, alpha=1, lambda=c(1,10,1))
lasso.pre<-predict(lasso_mod, s=best.lasso, xtest)
lasso.mse<-mean((test[,response]-lasso.pre)**2)
#We get a MSE of 0.173 which is the lowest error we've got so far
#predict(lasso_mod, type="coefficients", s=best.lasso)
# 10. Run ridge regression on the data set. Briefly discuss how you determined
# the appropriate tuning parameters.
#We use cross-validation to choose the optimal value of lambda
cv.ridge <-cv.glmnet (x, y ,alpha =0)
#plot(cv.ridge)
best.ridge<-cv.ridge$lambda.min #The the best value of lambda is 0.028
#Now we can compute the MSE
ridge_mod<-glmnet(x, y, family='binomial', alpha =0)
ridge.pre<-predict(ridge_mod, s=best.ridge, xtest)
ridge.mse<-mean((test[,response]-ridge.pre)**2)
#In this case the MSE is quite high (3.45) compare to the rest of the results
#predict(ridge_mod, type="coefficients", s=best.ridge)
#If we observe the coefficients some of them are very small
# 11. Run naive bayes on the data set.
library('naivebayes')
nb<-naive_bayes(f, data=train)
predict_nb<-predict(nb, test)
predict_nb<-as.logical(predict_nb)
confusionMatrix(predict_nb, test$default_payment_next_month)
#Applying Naive Bayes, there is a 59% accuracy, which is worse than the No information
#rate so this model is worst than the default case
naive.mse<-mean((test[,response]-predict_nb)**2)
#It has a mse of 0.407
# 12. Build a decision tree to classify the customers as defaulted
# or not-defaulted. Plot the resulting tree. Discuss whether you
# feel this is a good model.
library(rpart)
library(rpart.plot)
tree<-rpart(default_payment_next_month~., method="class", data=train)
rpart.plot(tree, type=4, extra=104, fallen.leaves=TRUE, main="Full Tree")
summary(tree)
p<-predict(tree, test, type="class")
confusionMatrix(p, test$default_payment_next_month)
#From this we see that the most important variable is pay_0, this tree has only one level.
#We get an Accuracy of 82% which is higher than the No Information Rate.
#The recall and the precision are similar as the case before
p<-as.logical(p)
tree.mse<-mean((test[,response]-p)**2)
#This has a mse of 0.178
# 13. Build a random forest model and apply it to classify the test data set.
library(randomForest)
train2<-train
train2$default_payment_next_month<-as.factor(train2$default_payment_next_month)
test2<-test
test2$default_payment_next_month<-as.factor(test2$default_payment_next_month)
rmodel<-randomForest(default_payment_next_month~.,train2, mtry=4, importance=T, ntree=500)
rpredict<-predict(rmodel, test2, type="class")
confusionMatrix(rpredict, test$default_payment_next_month)
rpredict<-as.logical(rpredict)
randomF.mse<-mean((test[,response]-rpredict)**2)
#we get a mse of 0.1793
#mtry <- tuneRF(train2[,var], train2[,response], ntreeTry=500, stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
# 14. Discuss the comparative performance of all of the models used. How should
# we determine which is best? Provide justification for your answer.
#If we compare the mse we see that the
logmodel$aic
confusionMatrix(defaulted, test$default_payment_next_month)
logpred<-predict(logmodel, test, type="response")
plot(cv.lasso)
sum(which(lasso.pre<0))
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[lasso.pre>threshold]<-1
defaulted<-as.logical(defaulted)
mean(defaulted) #We get that the mean in the defaulted is 12%
confusionMatrix(defaulted, test$default_payment_next_month)
confusionMatrix(rpredict, test$default_payment_next_month)
confusionMatrix(p, test$default_payment_next_month)
lasso_mod<-glmnet(x, y, alpha=1)
lasso.pre<-predict(lasso_mod, s=best.lasso, xtest)
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[lasso.pre>threshold]<-1
defaulted<-as.logical(defaulted)
mean(defaulted) #We get that the mean in the defaulted is 12%
confusionMatrix(defaulted, test$default_payment_next_month)
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[ridge.pre>threshold]<-1
defaulted<-as.logical(defaulted)
confusionMatrix(defaulted, test$default_payment_next_month)
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[logpred>threshold]<-1
defaulted<-as.logical(defaulted)
table(defaulted, test$default_payment_next_month)
#We can see in this table the distribution of how our model predicted the outcome
mean(defaulted)
#we obtained that only 12% of the cases would we yes, when we know that the percentage of "yes"
#in the data is 22%
mean(test$default_payment_next_month)
#The mean of "yes" is in both cases really small, since there are low number of examples that
#has 1 as an outcome.
confusionMatrix(defaulted, test$default_payment_next_month)
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[forpred>threshold]<-1
defaulted<-as.logical(defaulted)
confusionMatrix(defaulted, test$default_payment_next_month)
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[backpred>threshold]<-1
defaulted<-as.logical(defaulted)
confusionMatrix(defaulted, test$default_payment_next_month)
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[lasso.pre>threshold]<-1
defaulted<-as.logical(defaulted)
confusionMatrix(defaulted, test$default_payment_next_month)
# 11. Run naive bayes on the data set.
library('naivebayes')
threshold<-0.5
defaulted<-rep(0, length(test$default_payment_next_month))
defaulted[ridge.pre>threshold]<-1
defaulted<-as.logical(defaulted)
confusionMatrix(defaulted, test$default_payment_next_month)
p<-predict(tree, test, type="class")
confusionMatrix(p, test$default_payment_next_month)
rmodel<-randomForest(default_payment_next_month~.,train2, mtry=4, importance=T, ntree=500)
rpredict<-predict(rmodel, test2, type="class")
confusionMatrix(rpredict, test$default_payment_next_month)
summary(credit)
ggplot(data=credit_clean, aes(x=sex, y=default_payment_next_month)) +
geom_bar(stat="identity")
ggplot(data=credit_clean, aes(x=sex, y=default_payment_next_month, fill=supp)) +
geom_bar(stat="identity", position=position_dodge())
ggplot(data=credit_clean, aes(x=sex, y=default_payment_next_month)) +
geom_bar(stat="identity", position=position_dodge())
ggplot(data=credit_clean, aes(x=sex, y=default_payment_next_month, fill=default_payment_next_month)) +
geom_bar(stat="identity", position=position_dodge())
ggplot(data=credit_clean, aes(x=sex, fill=default_payment_next_month)) +
geom_bar(stat="identity", position=position_dodge())
ggplot(data=credit_clean, aes(x=sex )) +
geom_bar(stat="identity", position=position_dodge(), fill=default_payment_next_month)
ggplot(data=credit_clean, aes(x=sex )) +
geom_bar(stat="identity", position=position_dodge(), fill="default_payment_next_month")
View(credit_clean)
ggplot(data=credit_clean, aes(x=sex, y=default_payment_next_month, fill=default_payment_next_month)) +
geom_bar(stat="count", position=position_dodge())
ggplot(data=credit_clean, aes(x=sex, fill=default_payment_next_month)) +
geom_bar(stat="count", position=position_dodge())
n<-which(credit_clean$sex=='female')
mean(credit_clean$default_payment_next_month[n])
mean(credit_clean$default_payment_next_month[credit_clean$sex=='male'])
ggplot(data=credit_clean, aes(x=education, fill=default_payment_next_month)) +
geom_bar(stat="count", position=position_dodge())
mean(credit_clean$default_payment_next_month[credit_clean$education=='university'])
summary(credit_clean)
ggplot(data=credit_clean, aes(x=marriage, fill=default_payment_next_month)) +
geom_bar(stat="count", position=position_dodge())
ggplot(data=credit_clean, geom="histogram")
ggplot(data=credit_clean$limit_bal, geom="histogram")
ggplot(credit_clean, aes(x = default_payment_next_month , y = limit_bal, color=default_payment_next_month)) +
geom_boxplot()
